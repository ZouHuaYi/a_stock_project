#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
è‚¡ç¥¨æ–°é—»åˆ†æå™¨

åŸºäºGoogleæœç´¢APIè·å–è‚¡ç¥¨ç›¸å…³æ–°é—»ï¼Œå¹¶è¿›è¡Œåˆ†æ
"""

import os
import logging
from typing import Dict, List, Any, Optional, Tuple

from analyzer.base_analyzer import BaseAnalyzer
from utils.google_api import GoogleSearchAPI, get_google_search_api

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class NewsAnalyzer(BaseAnalyzer):
    """
    è‚¡ç¥¨æ–°é—»åˆ†æå™¨
    
    ä½¿ç”¨Googleæœç´¢APIè·å–è‚¡ç¥¨ç›¸å…³æ–°é—»ï¼Œå¹¶è¿›è¡Œåˆ†æ
    """
    
    def __init__(self, **kwargs):
        """
        åˆå§‹åŒ–è‚¡ç¥¨æ–°é—»åˆ†æå™¨
        
        å‚æ•°:
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™çˆ¶ç±»
        """
        super().__init__(**kwargs)
        
        # åˆå§‹åŒ–Googleæœç´¢API
        self.search_api = get_google_search_api()
        
        # æ£€æŸ¥APIæ˜¯å¦å¯ç”¨
        if not self.search_api.api_key or not self.search_api.cx:
            logger.warning("Googleæœç´¢APIæœªé…ç½®ï¼Œè¯·è®¾ç½®GOOGLE_API_KEYå’ŒGOOGLE_SEARCH_CXç¯å¢ƒå˜é‡")
            
        # æœç´¢é…ç½®
        self.default_max_results = kwargs.get('max_news_results', 10)
        self.default_days = kwargs.get('news_days', 7)
        self.default_sites = kwargs.get('news_sites', [
            'finance.sina.com.cn',
            'finance.eastmoney.com',
            'finance.qq.com',
            'business.sohu.com',
            'money.163.com'
        ])
        
        # æ·±åº¦çˆ¬å–é…ç½®
        self.enable_deep_crawl = kwargs.get('enable_deep_crawl', True)
        self.deep_crawl_limit = kwargs.get('deep_crawl_limit', 3)

    def analyze(self, stock_code: str, stock_name: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """
        åˆ†æè‚¡ç¥¨ç›¸å…³æ–°é—»
        
        å‚æ•°:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™åªæœç´¢è‚¡ç¥¨ä»£ç 
            **kwargs: å…¶ä»–å‚æ•°
        
        è¿”å›:
            åˆ†æç»“æœå­—å…¸ï¼ŒåŒ…å«æ–°é—»åˆ—è¡¨ã€æƒ…æ„Ÿåˆ†æç­‰
        """
        logger.info(f"åˆ†æè‚¡ç¥¨ {stock_code} {stock_name or ''} çš„ç›¸å…³æ–°é—»")
        
        # æ£€æŸ¥APIæ˜¯å¦å¯ç”¨
        if not self.search_api.api_key or not self.search_api.cx:
            return {
                'status': 'error',
                'message': 'Googleæœç´¢APIæœªé…ç½®',
                'news': []
            }
        
        # è·å–æœç´¢å‚æ•°
        max_results = kwargs.get('max_results', self.default_max_results)
        days = kwargs.get('days', self.default_days)
        sites = kwargs.get('sites', self.default_sites)
        
        # è·å–æ·±åº¦çˆ¬å–å‚æ•°
        deep_crawl = kwargs.get('deep_crawl', self.enable_deep_crawl)
        deep_crawl_limit = kwargs.get('deep_crawl_limit', self.deep_crawl_limit)
        
        try:
            # è·å–è‚¡ç¥¨ç›¸å…³æ–°é—»
            news_results = self.search_api.search_stock_info(
                stock_code=stock_code,
                stock_name=stock_name,
                max_results=max_results,
                days=days,
                site_list=sites,
                deep_crawl=deep_crawl,
                deep_crawl_limit=deep_crawl_limit
            )
            
            # å¯¹æ–°é—»è¿›è¡Œç®€å•åˆ†æ
            sentiment, keyword_stats, content_summary = self._analyze_news_content(news_results)
            
            return {
                'status': 'success',
                'message': f'æ‰¾åˆ° {len(news_results)} æ¡ç›¸å…³æ–°é—»',
                'news': news_results,
                'sentiment': sentiment,
                'keyword_stats': keyword_stats,
                'content_summary': content_summary,
                'has_deep_content': any('extracted_content' in item for item in news_results)
            }
            
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨æ–°é—»æ—¶å‡ºé”™: {str(e)}")
            return {
                'status': 'error',
                'message': f'è·å–æ–°é—»å¤±è´¥: {str(e)}',
                'news': []
            }
    
    def _analyze_news_content(self, news_results: List[Dict[str, Any]]) -> Tuple[str, Dict[str, int], str]:
        """
        ç®€å•åˆ†ææ–°é—»å†…å®¹ï¼Œæå–å…³é”®è¯å’Œæƒ…æ„Ÿ
        
        å‚æ•°:
            news_results: æ–°é—»ç»“æœåˆ—è¡¨
            
        è¿”å›:
            (æƒ…æ„Ÿè¯„ä¼°, å…³é”®è¯ç»Ÿè®¡, å†…å®¹æ€»ç»“)
        """
        if not news_results:
            return "neutral", {}, ""
            
        # æ­£é¢è¯æ±‡
        positive_words = [
            'ä¸Šæ¶¨', 'å¢é•¿', 'ç›ˆåˆ©', 'åˆ©å¥½', 'çªç ´', 'çœ‹å¥½', 'å¼ºåŠ¿', 
            'æœºä¼š', 'ç‰›å¸‚', 'åå¼¹', 'å›å‡', 'è·åˆ©', 'å¢æŒ', 'æ¨è'
        ]
        
        # è´Ÿé¢è¯æ±‡
        negative_words = [
            'ä¸‹è·Œ', 'äºæŸ', 'åˆ©ç©º', 'è·Œç ´', 'çœ‹ç©º', 'å¼±åŠ¿',
            'é£é™©', 'ç†Šå¸‚', 'ä¸‹æ»‘', 'å›è½', 'äºæŸ', 'å‡æŒ', 'è°¨æ…'
        ]
        
        # ç»Ÿè®¡è¯é¢‘
        keyword_stats = {}
        positive_count = 0
        negative_count = 0
        
        # å†…å®¹æ€»ç»“
        content_summary = ""
        has_deep_content = False
        
        # åˆ†ææ‰€æœ‰æ–°é—»
        for news in news_results:
            # é¦–å…ˆä½¿ç”¨æ ‡é¢˜å’Œæ‘˜è¦ï¼ˆåŸºæœ¬æœç´¢ä¿¡æ¯ï¼‰
            text = (news.get('title', '') + ' ' + news.get('snippet', '')).lower()
            
            # å¦‚æœæœ‰æ·±åº¦çˆ¬å–çš„å†…å®¹ï¼Œæ·»åŠ åˆ°åˆ†ææ–‡æœ¬ä¸­
            if 'extracted_content' in news and news.get('extraction_success', False):
                has_deep_content = True
                extracted_content = news.get('extracted_content', '')
                if extracted_content:
                    text += ' ' + extracted_content.lower()
            
            # ç»Ÿè®¡æ­£é¢è¯æ±‡
            for word in positive_words:
                if word in text:
                    positive_count += 1
                    keyword_stats[word] = keyword_stats.get(word, 0) + 1
                    
            # ç»Ÿè®¡è´Ÿé¢è¯æ±‡
            for word in negative_words:
                if word in text:
                    negative_count += 1
                    keyword_stats[word] = keyword_stats.get(word, 0) + 1
        
        # ç¡®å®šæ•´ä½“æƒ…æ„Ÿ
        if positive_count > negative_count * 1.5:
            sentiment = "positive"
        elif negative_count > positive_count * 1.5:
            sentiment = "negative"
        else:
            sentiment = "neutral"
        
        # åˆ›å»ºå†…å®¹æ€»ç»“
        if has_deep_content:
            # æ•´ç†ä¸€ä¸ªç®€å•çš„å†…å®¹æ€»ç»“
            word_count = sum(len(news.get('extracted_content', '').split()) for news in news_results if 'extracted_content' in news)
            content_summary = f"å·²æ·±åº¦çˆ¬å–{sum(1 for n in news_results if 'extracted_content' in n)}ç¯‡æ–°é—»ï¼Œå…±{word_count}ä¸ªè¯ã€‚"
            
            # æ·»åŠ æœ€é•¿æ–‡ç« çš„æ ‡é¢˜
            longest_article = max(
                [n for n in news_results if 'extracted_content' in n], 
                key=lambda x: len(x.get('extracted_content', '')),
                default=None
            )
            if longest_article:
                content_summary += f"\næœ€è¯¦ç»†çš„æ–‡ç« ï¼š{longest_article.get('title', '')}"
            
        return sentiment, keyword_stats, content_summary
    
    def get_related_keywords(self, stock_code: str, stock_name: Optional[str] = None) -> List[str]:
        """
        è·å–è‚¡ç¥¨ç›¸å…³å…³é”®è¯
        
        å‚æ•°:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            
        è¿”å›:
            å…³é”®è¯åˆ—è¡¨
        """
        default_keywords = ["è´¢æŠ¥", "ä¸šç»©", "åˆ©æ¶¦", "è¥æ”¶", "æŠ•èµ„è€…", "åˆ†æå¸ˆ"]
        
        if not stock_name:
            return default_keywords
            
        # ä½¿ç”¨è‚¡ç¥¨åç§°æ„å»ºå…³é”®è¯
        industry_keywords = []
        
        # æ ¹æ®è‚¡ç¥¨åç§°çŒœæµ‹è¡Œä¸š
        if "é“¶è¡Œ" in stock_name:
            industry_keywords = ["å­˜æ¬¾", "è´·æ¬¾", "ä¸è‰¯èµ„äº§", "å‡€æ¯å·®", "é‡‘è"]
        elif "ä¿é™©" in stock_name:
            industry_keywords = ["ä¿è´¹", "èµ”ä»˜ç‡", "æŠ•èµ„æ”¶ç›Š", "å‡†å¤‡é‡‘", "é‡‘è"]
        elif "ç§‘æŠ€" in stock_name or "ç”µå­" in stock_name:
            industry_keywords = ["ç ”å‘", "åˆ›æ–°", "ä¸“åˆ©", "æŠ€æœ¯", "æ•°å­—åŒ–"]
        elif "åŒ»è¯" in stock_name or "ç”Ÿç‰©" in stock_name:
            industry_keywords = ["æ–°è¯", "ç ”å‘", "æ‰¹å‡†", "ä¸´åºŠ", "åŒ»ä¿"]
        elif "åœ°äº§" in stock_name or "æˆ¿" in stock_name:
            industry_keywords = ["é”€å”®é¢", "åœŸåœ°", "æ¥¼å¸‚", "æ”¿ç­–", "è°ƒæ§"]
        
        return default_keywords + industry_keywords
    
    def format_output(self, result: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–è¾“å‡ºåˆ†æç»“æœ
        
        å‚æ•°:
            result: åˆ†æç»“æœå­—å…¸
            
        è¿”å›:
            æ ¼å¼åŒ–çš„è¾“å‡ºå­—ç¬¦ä¸²
        """
        if result['status'] == 'error':
            return f"æ–°é—»åˆ†æå¤±è´¥: {result['message']}"
            
        output = f"æ–°é—»åˆ†æç»“æœ: æ‰¾åˆ°{len(result['news'])}æ¡ç›¸å…³æ–°é—»\n"
        
        # æ·»åŠ å†…å®¹æ€»ç»“ï¼ˆå¦‚æœæœ‰æ·±åº¦çˆ¬å–ï¼‰
        if result.get('content_summary'):
            output += f"å†…å®¹åˆ†æ: {result['content_summary']}\n"
            
        output += f"æ•´ä½“æƒ…æ„Ÿ: "
        
        sentiment = result.get('sentiment', 'neutral')
        if sentiment == 'positive':
            output += "æ­£é¢ ğŸ“ˆ\n"
        elif sentiment == 'negative':
            output += "è´Ÿé¢ ğŸ“‰\n"
        else:
            output += "ä¸­æ€§ â¡ï¸\n"
            
        # æ·»åŠ å…³é”®è¯ç»Ÿè®¡
        keyword_stats = result.get('keyword_stats', {})
        if keyword_stats:
            output += "\nå…³é”®è¯å‡ºç°é¢‘ç‡:\n"
            for word, count in sorted(keyword_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
                output += f"- {word}: {count}æ¬¡\n"
                
        # æ·»åŠ æ–°é—»åˆ—è¡¨
        output += "\næœ€æ–°ç›¸å…³æ–°é—»:\n"
        for i, news in enumerate(result['news'][:5], 1):
            output += f"{i}. {news['title']}\n"
            output += f"   æ¥æº: {news['display_link']}\n"
            
            # å¦‚æœæœ‰æ·±åº¦çˆ¬å–çš„å†…å®¹ï¼Œæ·»åŠ å†…å®¹é¢„è§ˆ
            if 'extracted_content' in news and news.get('extraction_success', False):
                content = news.get('extracted_content', '')
                if content:
                    # æˆªå–å‰150ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ
                    preview = content[:150] + ('...' if len(content) > 150 else '')
                    output += f"   å†…å®¹é¢„è§ˆ: {preview}\n"
                    
            output += f"   é“¾æ¥: {news['link']}\n\n"
            
        return output
    
    def extract_single_news(self, url: str) -> Dict[str, Any]:
        """
        æå–å•ä¸ªæ–°é—»é¡µé¢çš„è¯¦ç»†å†…å®¹
        
        å‚æ•°:
            url: æ–°é—»URL
            
        è¿”å›:
            æå–çš„å†…å®¹å­—å…¸
        """
        return self.search_api.extract_financial_news_content(url) 