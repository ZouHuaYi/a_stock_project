#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
è‚¡ç¥¨æ–°é—»åˆ†æå™¨

åŸºäºGoogleæœç´¢APIè·å–è‚¡ç¥¨ç›¸å…³æ–°é—»ï¼Œå¹¶è¿›è¡Œåˆ†æ
"""

import os
import logging
import traceback
from typing import Dict, List, Any, Optional, Tuple

from analyzer.base_analyzer import BaseAnalyzer

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥jiebaåˆ†è¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æä¾›å…¼å®¹å®ç°
try:
    import jieba
    logger.debug("æˆåŠŸå¯¼å…¥jieba")
except ImportError:
    logger.warning("jiebaæ¨¡å—æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•çš„åˆ†è¯æ–¹æ³•ä»£æ›¿")
    
    class DummyJieba:
        """æ¨¡æ‹Ÿjiebaçš„ç®€å•åˆ†è¯å®ç°"""
        @staticmethod
        def cut(text, cut_all=False):
            """ç®€å•çš„æŒ‰ç©ºæ ¼åˆ†è¯"""
            return text.split()
            
    jieba = DummyJieba()

# å°è¯•å¯¼å…¥wordcloudï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å¿½ç•¥è¯äº‘ç”Ÿæˆ
try:
    import wordcloud
    HAS_WORDCLOUD = True
    logger.debug("æˆåŠŸå¯¼å…¥wordcloud")
except ImportError:
    HAS_WORDCLOUD = False
    logger.warning("wordcloudæ¨¡å—æœªå®‰è£…ï¼Œè¯äº‘ç”ŸæˆåŠŸèƒ½å°†è¢«ç¦ç”¨")

# å°è¯•å¯¼å…¥GoogleSearchAPIï¼Œå¦‚æœå¤±è´¥æä¾›ä¸€ä¸ªç©ºçš„æ›¿ä»£ç±»
try:
    from utils.google_api import GoogleSearchAPI, get_google_search_api
    logger.debug("æˆåŠŸå¯¼å…¥GoogleSearchAPI")
except Exception as e:
    logger.error(f"å¯¼å…¥GoogleSearchAPIå¤±è´¥: {str(e)}")
    logger.error(traceback.format_exc())
    
    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„APIç±»
    class DummyGoogleSearchAPI:
        """GoogleSearchAPIçš„æ›¿ä»£ç±»ï¼Œç”¨äºå¤„ç†å¯¼å…¥å¤±è´¥æƒ…å†µ"""
        def __init__(self, *args, **kwargs):
            self.api_key = None
            self.cx = None
            logger.warning("ä½¿ç”¨DummyGoogleSearchAPIæ›¿ä»£ï¼Œæ‰€æœ‰APIè°ƒç”¨å°†è¿”å›ç©ºç»“æœ")
            
        def search_stock_info(self, *args, **kwargs):
            """æ¨¡æ‹Ÿæœç´¢æ–¹æ³•"""
            return []
            
        def extract_financial_news_content(self, *args, **kwargs):
            """æ¨¡æ‹Ÿå†…å®¹æå–æ–¹æ³•"""
            return {"status": "error", "error": "GoogleSearchAPIä¸å¯ç”¨"}
    
    def get_google_search_api():
        """è·å–æ¨¡æ‹ŸAPIå®ä¾‹"""
        return DummyGoogleSearchAPI()

class NewsAnalyzer(BaseAnalyzer):
    """
    è‚¡ç¥¨æ–°é—»åˆ†æå™¨
    
    ä½¿ç”¨Googleæœç´¢APIè·å–è‚¡ç¥¨ç›¸å…³æ–°é—»ï¼Œå¹¶è¿›è¡Œåˆ†æ
    """
    
    def __init__(self, **kwargs):
        """
        åˆå§‹åŒ–è‚¡ç¥¨æ–°é—»åˆ†æå™¨
        
        å‚æ•°:
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™çˆ¶ç±»
        """
        # ä»kwargsä¸­æå–BaseAnalyzeréœ€è¦çš„å‚æ•°
        stock_code = kwargs.get('stock_code')
        stock_name = kwargs.get('stock_name')
        end_date = kwargs.get('end_date')
        days = kwargs.get('days')
        start_date = kwargs.get('start_date')
        
        logger.info(f"NewsAnalyzeråˆå§‹åŒ–å‚æ•°: stock_code={stock_code}, days={days}")
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•ï¼Œåªä¼ é€’çˆ¶ç±»éœ€è¦çš„å‚æ•°
        try:
            super().__init__(
                stock_code=stock_code,
                stock_name=stock_name,
                end_date=end_date,
                days=days,
                start_date=start_date
            )
            logger.info("BaseAnalyzeråˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"BaseAnalyzeråˆå§‹åŒ–å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            raise
        
        # åˆå§‹åŒ–Googleæœç´¢API
        try:
            self.search_api = get_google_search_api()
            logger.info("Googleæœç´¢APIåˆå§‹åŒ–å®Œæˆ")
            
            # æ£€æŸ¥APIæ˜¯å¦å¯ç”¨
            if not self.search_api.api_key or not self.search_api.cx:
                logger.warning("Googleæœç´¢APIæœªé…ç½®ï¼Œè¯·è®¾ç½®GOOGLE_API_KEYå’ŒGOOGLE_SEARCH_CXç¯å¢ƒå˜é‡")
        except Exception as e:
            logger.error(f"Googleæœç´¢APIåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            self.search_api = DummyGoogleSearchAPI()
            
        # æœç´¢é…ç½®
        self.default_max_results = kwargs.get('max_news_results', 10)
        self.default_days = kwargs.get('news_days', 7)
        self.default_sites = kwargs.get('news_sites', [
            'finance.sina.com.cn',
            'finance.eastmoney.com',
            'finance.qq.com',
            'business.sohu.com',
            'money.163.com'
        ])
        
        # æ·±åº¦çˆ¬å–é…ç½®
        self.enable_deep_crawl = kwargs.get('enable_deep_crawl', True)
        self.deep_crawl_limit = kwargs.get('deep_crawl_limit', 3)
        
        logger.info(f"NewsAnalyzeråˆå§‹åŒ–å®Œæˆ: max_results={self.default_max_results}, deep_crawl={self.enable_deep_crawl}")

    def analyze(self, stock_code: str, stock_name: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """
        åˆ†æè‚¡ç¥¨ç›¸å…³æ–°é—»
        
        å‚æ•°:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™åªæœç´¢è‚¡ç¥¨ä»£ç 
            **kwargs: å…¶ä»–å‚æ•°
        
        è¿”å›:
            åˆ†æç»“æœå­—å…¸ï¼ŒåŒ…å«æ–°é—»åˆ—è¡¨ã€æƒ…æ„Ÿåˆ†æç­‰
        """
        logger.info(f"åˆ†æè‚¡ç¥¨ {stock_code} {stock_name or ''} çš„ç›¸å…³æ–°é—»")
        
        # æ£€æŸ¥APIæ˜¯å¦å¯ç”¨
        if not self.search_api.api_key or not self.search_api.cx:
            return {
                'status': 'error',
                'message': 'Googleæœç´¢APIæœªé…ç½®',
                'news': []
            }
        
        # è·å–æœç´¢å‚æ•°
        max_results = kwargs.get('max_results', self.default_max_results)
        days = kwargs.get('days', self.default_days)
        sites = kwargs.get('sites', self.default_sites)
        
        # è·å–æ·±åº¦çˆ¬å–å‚æ•°
        deep_crawl = kwargs.get('deep_crawl', self.enable_deep_crawl)
        deep_crawl_limit = kwargs.get('deep_crawl_limit', self.deep_crawl_limit)
        
        try:
            # è·å–è‚¡ç¥¨ç›¸å…³æ–°é—»
            news_results = self.search_api.search_stock_info(
                stock_code=stock_code,
                stock_name=stock_name,
                max_results=max_results,
                days=days,
                site_list=sites,
                deep_crawl=deep_crawl,
                deep_crawl_limit=deep_crawl_limit
            )
            
            # å¯¹æ–°é—»è¿›è¡Œç®€å•åˆ†æ
            sentiment, keyword_stats, content_summary = self._analyze_news_content(news_results)
            
            return {
                'status': 'success',
                'message': f'æ‰¾åˆ° {len(news_results)} æ¡ç›¸å…³æ–°é—»',
                'news': news_results,
                'sentiment': sentiment,
                'keyword_stats': keyword_stats,
                'content_summary': content_summary,
                'has_deep_content': any('extracted_content' in item for item in news_results)
            }
            
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨æ–°é—»æ—¶å‡ºé”™: {str(e)}")
            return {
                'status': 'error',
                'message': f'è·å–æ–°é—»å¤±è´¥: {str(e)}',
                'news': []
            }
    
    def _analyze_news_content(self, news_results: List[Dict[str, Any]]) -> Tuple[str, Dict[str, int], str]:
        """
        ç®€å•åˆ†ææ–°é—»å†…å®¹ï¼Œæå–å…³é”®è¯å’Œæƒ…æ„Ÿ
        
        å‚æ•°:
            news_results: æ–°é—»ç»“æœåˆ—è¡¨
            
        è¿”å›:
            (æƒ…æ„Ÿè¯„ä¼°, å…³é”®è¯ç»Ÿè®¡, å†…å®¹æ€»ç»“)
        """
        if not news_results:
            return "neutral", {}, ""
            
        # æ­£é¢è¯æ±‡
        positive_words = [
            'ä¸Šæ¶¨', 'å¢é•¿', 'ç›ˆåˆ©', 'åˆ©å¥½', 'çªç ´', 'çœ‹å¥½', 'å¼ºåŠ¿', 
            'æœºä¼š', 'ç‰›å¸‚', 'åå¼¹', 'å›å‡', 'è·åˆ©', 'å¢æŒ', 'æ¨è'
        ]
        
        # è´Ÿé¢è¯æ±‡
        negative_words = [
            'ä¸‹è·Œ', 'äºæŸ', 'åˆ©ç©º', 'è·Œç ´', 'çœ‹ç©º', 'å¼±åŠ¿',
            'é£é™©', 'ç†Šå¸‚', 'ä¸‹æ»‘', 'å›è½', 'äºæŸ', 'å‡æŒ', 'è°¨æ…'
        ]
        
        # ç»Ÿè®¡è¯é¢‘
        keyword_stats = {}
        positive_count = 0
        negative_count = 0
        
        # å†…å®¹æ€»ç»“
        content_summary = ""
        has_deep_content = False
        
        # åˆ†ææ‰€æœ‰æ–°é—»
        for news in news_results:
            # é¦–å…ˆä½¿ç”¨æ ‡é¢˜å’Œæ‘˜è¦ï¼ˆåŸºæœ¬æœç´¢ä¿¡æ¯ï¼‰
            text = (news.get('title', '') + ' ' + news.get('snippet', '')).lower()
            
            # å¦‚æœæœ‰æ·±åº¦çˆ¬å–çš„å†…å®¹ï¼Œæ·»åŠ åˆ°åˆ†ææ–‡æœ¬ä¸­
            if 'extracted_content' in news and news.get('extraction_success', False):
                has_deep_content = True
                extracted_content = news.get('extracted_content', '')
                if extracted_content:
                    text += ' ' + extracted_content.lower()
            
            # ç»Ÿè®¡æ­£é¢è¯æ±‡
            for word in positive_words:
                if word in text:
                    positive_count += 1
                    keyword_stats[word] = keyword_stats.get(word, 0) + 1
                    
            # ç»Ÿè®¡è´Ÿé¢è¯æ±‡
            for word in negative_words:
                if word in text:
                    negative_count += 1
                    keyword_stats[word] = keyword_stats.get(word, 0) + 1
        
        # ç¡®å®šæ•´ä½“æƒ…æ„Ÿ
        if positive_count > negative_count * 1.5:
            sentiment = "positive"
        elif negative_count > positive_count * 1.5:
            sentiment = "negative"
        else:
            sentiment = "neutral"
        
        # åˆ›å»ºå†…å®¹æ€»ç»“
        if has_deep_content:
            # æ•´ç†ä¸€ä¸ªç®€å•çš„å†…å®¹æ€»ç»“
            word_count = sum(len(news.get('extracted_content', '').split()) for news in news_results if 'extracted_content' in news)
            content_summary = f"å·²æ·±åº¦çˆ¬å–{sum(1 for n in news_results if 'extracted_content' in n)}ç¯‡æ–°é—»ï¼Œå…±{word_count}ä¸ªè¯ã€‚"
            
            # æ·»åŠ æœ€é•¿æ–‡ç« çš„æ ‡é¢˜
            longest_article = max(
                [n for n in news_results if 'extracted_content' in n], 
                key=lambda x: len(x.get('extracted_content', '')),
                default=None
            )
            if longest_article:
                content_summary += f"\næœ€è¯¦ç»†çš„æ–‡ç« ï¼š{longest_article.get('title', '')}"
            
        return sentiment, keyword_stats, content_summary
    
    def get_related_keywords(self, stock_code: str, stock_name: Optional[str] = None) -> List[str]:
        """
        è·å–è‚¡ç¥¨ç›¸å…³å…³é”®è¯
        
        å‚æ•°:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            
        è¿”å›:
            å…³é”®è¯åˆ—è¡¨
        """
        default_keywords = ["è´¢æŠ¥", "ä¸šç»©", "åˆ©æ¶¦", "è¥æ”¶", "æŠ•èµ„è€…", "åˆ†æå¸ˆ"]
        
        if not stock_name:
            return default_keywords
            
        # ä½¿ç”¨è‚¡ç¥¨åç§°æ„å»ºå…³é”®è¯
        industry_keywords = []
        
        # æ ¹æ®è‚¡ç¥¨åç§°çŒœæµ‹è¡Œä¸š
        if "é“¶è¡Œ" in stock_name:
            industry_keywords = ["å­˜æ¬¾", "è´·æ¬¾", "ä¸è‰¯èµ„äº§", "å‡€æ¯å·®", "é‡‘è"]
        elif "ä¿é™©" in stock_name:
            industry_keywords = ["ä¿è´¹", "èµ”ä»˜ç‡", "æŠ•èµ„æ”¶ç›Š", "å‡†å¤‡é‡‘", "é‡‘è"]
        elif "ç§‘æŠ€" in stock_name or "ç”µå­" in stock_name:
            industry_keywords = ["ç ”å‘", "åˆ›æ–°", "ä¸“åˆ©", "æŠ€æœ¯", "æ•°å­—åŒ–"]
        elif "åŒ»è¯" in stock_name or "ç”Ÿç‰©" in stock_name:
            industry_keywords = ["æ–°è¯", "ç ”å‘", "æ‰¹å‡†", "ä¸´åºŠ", "åŒ»ä¿"]
        elif "åœ°äº§" in stock_name or "æˆ¿" in stock_name:
            industry_keywords = ["é”€å”®é¢", "åœŸåœ°", "æ¥¼å¸‚", "æ”¿ç­–", "è°ƒæ§"]
        
        return default_keywords + industry_keywords
    
    def format_output(self, result: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–è¾“å‡ºåˆ†æç»“æœ
        
        å‚æ•°:
            result: åˆ†æç»“æœå­—å…¸
            
        è¿”å›:
            æ ¼å¼åŒ–çš„è¾“å‡ºå­—ç¬¦ä¸²
        """
        if result['status'] == 'error':
            return f"æ–°é—»åˆ†æå¤±è´¥: {result['message']}"
            
        output = f"æ–°é—»åˆ†æç»“æœ: æ‰¾åˆ°{len(result['news'])}æ¡ç›¸å…³æ–°é—»\n"
        
        # æ·»åŠ å†…å®¹æ€»ç»“ï¼ˆå¦‚æœæœ‰æ·±åº¦çˆ¬å–ï¼‰
        if result.get('content_summary'):
            output += f"å†…å®¹åˆ†æ: {result['content_summary']}\n"
            
        output += f"æ•´ä½“æƒ…æ„Ÿ: "
        
        sentiment = result.get('sentiment', 'neutral')
        if sentiment == 'positive':
            output += "æ­£é¢ ğŸ“ˆ\n"
        elif sentiment == 'negative':
            output += "è´Ÿé¢ ğŸ“‰\n"
        else:
            output += "ä¸­æ€§ â¡ï¸\n"
            
        # æ·»åŠ å…³é”®è¯ç»Ÿè®¡
        keyword_stats = result.get('keyword_stats', {})
        if keyword_stats:
            output += "\nå…³é”®è¯å‡ºç°é¢‘ç‡:\n"
            for word, count in sorted(keyword_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
                output += f"- {word}: {count}æ¬¡\n"
                
        # æ·»åŠ æ–°é—»åˆ—è¡¨
        output += "\næœ€æ–°ç›¸å…³æ–°é—»:\n"
        for i, news in enumerate(result['news'][:5], 1):
            output += f"{i}. {news['title']}\n"
            output += f"   æ¥æº: {news['display_link']}\n"
            
            # å¦‚æœæœ‰æ·±åº¦çˆ¬å–çš„å†…å®¹ï¼Œæ·»åŠ å†…å®¹é¢„è§ˆ
            if 'extracted_content' in news and news.get('extraction_success', False):
                content = news.get('extracted_content', '')
                if content:
                    # æˆªå–å‰150ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ
                    preview = content[:150] + ('...' if len(content) > 150 else '')
                    output += f"   å†…å®¹é¢„è§ˆ: {preview}\n"
                    
            output += f"   é“¾æ¥: {news['link']}\n\n"
            
        return output
    
    def extract_single_news(self, url: str) -> Dict[str, Any]:
        """
        æå–å•ä¸ªæ–°é—»é¡µé¢çš„è¯¦ç»†å†…å®¹
        
        å‚æ•°:
            url: æ–°é—»URL
            
        è¿”å›:
            æå–çš„å†…å®¹å­—å…¸
        """
        return self.search_api.extract_financial_news_content(url) 
    
    def fetch_data(self) -> bool:
        """
        è·å–æ•°æ®ï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–çš„æ•°æ®è·å–ï¼Œç›´æ¥è¿”å›True
        
        è¿”å›:
            bool: æ˜¯å¦æˆåŠŸè·å–æ•°æ®
        """
        logger.info(f"NewsAnalyzer ä¸éœ€è¦é¢å¤–çš„æ•°æ®è·å–ï¼Œç›´æ¥è¿”å›True")
        return True
    
    def process_single_url(self, url: str, save_path=None) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªURLæå–å‘½ä»¤å¹¶æ ¼å¼åŒ–è¾“å‡º
        
        å‚æ•°:
            url: éœ€è¦æå–çš„URL
            save_path: ä¿å­˜ç»“æœçš„è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä¿å­˜
            
        è¿”å›:
            å¤„ç†ç»“æœå­—å…¸
        """
        logger.info(f"æå–å•ä¸ªæ–°é—»URL: {url}")
        
        # æå–æ–°é—»å†…å®¹
        content_data = self.extract_single_news(url)
        
        result = {'status': 'error', 'message': 'æå–å¤±è´¥'}
        
        if 'data' in content_data:
            # è¾“å‡ºæå–ç»“æœ
            data = content_data['data']
            output = f"æ–°é—»æå–ç»“æœ:\n"
            output += f"æ ‡é¢˜: {data.get('title', 'æœªæ‰¾åˆ°æ ‡é¢˜')}\n"
            output += f"å‘å¸ƒæ—¥æœŸ: {data.get('publish_date', 'æœªæ‰¾åˆ°æ—¥æœŸ')}\n"
            output += f"ä½œè€…/æ¥æº: {data.get('author', 'æœªæ‰¾åˆ°ä½œè€…')}\n"
            output += f"å…³é”®è¯: {data.get('keywords', 'æœªæ‰¾åˆ°å…³é”®è¯')}\n\n"
            output += f"å†…å®¹:\n{data.get('content', 'æœªæ‰¾åˆ°å†…å®¹')}\n"
            
            # æ·»åŠ åˆ°ç»“æœ
            result = {
                'status': 'success',
                'message': 'æå–æˆåŠŸ',
                'data': data,
                'formatted_output': output
            }
            
            # ä¿å­˜ç»“æœ
            if save_path:
                output_file = save_path
                if not output_file.endswith('.txt'):
                    output_file += '.txt'
                
                # ç¡®ä¿outputç›®å½•å­˜åœ¨
                os.makedirs('output', exist_ok=True)
                output_path = os.path.join('output', output_file)
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(output)
                    
                logger.info(f"æå–ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
                result['output_path'] = output_path
        else:
            # æå–å¤±è´¥
            error_msg = f"ä»URLæå–å†…å®¹å¤±è´¥: {url}"
            if 'error' in content_data:
                error_msg += f" - {content_data['error']}"
                
            logger.error(error_msg)
            result = {
                'status': 'error',
                'message': error_msg,
                'error': content_data.get('error', 'æœªçŸ¥é”™è¯¯')
            }
            
        return result
    
    def run_analysis(self, save_path=None, additional_context=None) -> dict:
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        
        å‚æ•°:
            save_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä¿å­˜ç»“æœ
            additional_context: é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¾‹å¦‚å…¶ä»–åˆ†æç»“æœ
            
        è¿”å›:
            åˆ†æç»“æœå­—å…¸
        """
        try:
            logger.info(f"NewsAnalyzerå¼€å§‹è¿è¡Œåˆ†æ: {self.stock_code}")
            
            # è·å–æ•°æ®
            logger.info("è°ƒç”¨fetch_dataæ–¹æ³•")
            if not self.fetch_data():
                logger.error("æ•°æ®è·å–å¤±è´¥")
                return {'status': 'error', 'message': 'æ•°æ®è·å–å¤±è´¥'}
            
            logger.info("å°è¯•è·å–è‚¡ç¥¨åç§°")
            # è·å–è‚¡ç¥¨åç§°ï¼Œç”¨äºæœç´¢
            try:
                from data.stock_data import StockData
                stock_data = StockData()
                logger.info(f"æˆåŠŸåˆ›å»ºStockDataå®ä¾‹")
                
                stock_info = stock_data.get_stock_info(self.stock_code)
                logger.info(f"è·å–åˆ°è‚¡ç¥¨ä¿¡æ¯: {stock_info}")
                
                stock_name = stock_info.get('name') if stock_info else None
                logger.info(f"è·å–åˆ°è‚¡ç¥¨åç§°: {stock_name}")
            except Exception as e:
                logger.error(f"è·å–è‚¡ç¥¨åç§°æ—¶å‡ºé”™: {str(e)}")
                logger.error(traceback.format_exc())
                stock_name = None
            
            # æ‰§è¡Œæ–°é—»åˆ†æ
            logger.info(f"å¼€å§‹æ‰§è¡Œæ–°é—»åˆ†æ: {self.stock_code} {stock_name}")
            try:
                result = self.analyze(
                    stock_code=self.stock_code, 
                    stock_name=stock_name,
                    max_results=self.default_max_results,
                    days=self.days, 
                    sites=self.default_sites,
                    deep_crawl=self.enable_deep_crawl,
                    deep_crawl_limit=self.deep_crawl_limit
                )
                logger.info(f"æ–°é—»åˆ†æå®Œæˆ: çŠ¶æ€ {result.get('status')}")
            except Exception as e:
                logger.error(f"æ‰§è¡Œæ–°é—»åˆ†ææ—¶å‡ºé”™: {str(e)}")
                logger.error(traceback.format_exc())
                return {'status': 'error', 'message': f'æ–°é—»åˆ†æå¤±è´¥: {str(e)}'}
            
            # ç”Ÿæˆæ ¼å¼åŒ–è¾“å‡º
            logger.info("ç”Ÿæˆæ ¼å¼åŒ–è¾“å‡º")
            try:
                output = self.format_output(result)
                logger.info("æ ¼å¼åŒ–è¾“å‡ºç”Ÿæˆå®Œæˆ")
            except Exception as e:
                logger.error(f"ç”Ÿæˆæ ¼å¼åŒ–è¾“å‡ºæ—¶å‡ºé”™: {str(e)}")
                logger.error(traceback.format_exc())
                output = f"æ ¼å¼åŒ–è¾“å‡ºå¤±è´¥: {str(e)}"
            
            # å¦‚æœéœ€è¦ä¿å­˜ç»“æœ
            if save_path:
                logger.info(f"ä¿å­˜åˆ†æç»“æœåˆ°: {save_path}")
                try:
                    output_file = save_path
                    if not output_file.endswith('.txt'):
                        output_file += '.txt'
                    
                    # ç¡®ä¿outputç›®å½•å­˜åœ¨
                    os.makedirs('output', exist_ok=True)
                    output_path = os.path.join('output', output_file)
                    
                    with open(output_path, 'w', encoding='utf-8') as f:
                        f.write(output)
                        # ä¿å­˜å®Œæ•´çš„æ–°é—»ä¿¡æ¯
                        f.write("\n\nå®Œæ•´æ–°é—»åˆ—è¡¨:\n")
                        for i, news in enumerate(result['news'], 1):
                            f.write(f"{i}. {news['title']}\n")
                            f.write(f"   é“¾æ¥: {news['link']}\n")
                            f.write(f"   æ‘˜è¦: {news['snippet']}\n\n")
                    
                    logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")
                    result['output_path'] = output_path
                except Exception as e:
                    logger.error(f"ä¿å­˜åˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")
                    logger.error(traceback.format_exc())
            
            # æ·»åŠ æ ¼å¼åŒ–è¾“å‡ºåˆ°ç»“æœä¸­
            result['formatted_output'] = output
            
            logger.info("NewsAnalyzeråˆ†æå®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"æ–°é—»åˆ†æå™¨è¿è¡Œå‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())
            return {'status': 'error', 'message': f'æ–°é—»åˆ†æå¤±è´¥: {str(e)}'}
    
